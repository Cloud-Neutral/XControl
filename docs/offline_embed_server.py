#!/usr/bin/env python3
"""
Offline Embedding Server for BGE-M3 (Flask + SentenceTransformers)
- ä»…è¿è¡Œï¼šä¸åšä¾èµ–å®‰è£…/ä¸‹è½½ï¼›è¯·å…ˆç”¨ docs/models_downloading.py ä¸‹è½½æ¨¡å‹
- macOS M4 Pro é»˜è®¤ä½¿ç”¨ Apple GPU (MPS)
- /v1/embeddings OpenAI å…¼å®¹è¾“å‡ºï¼ŒBGE-M3 é»˜è®¤ 1024 ç»´ï¼Œå·² L2 å½’ä¸€

Env (å¯é€‰):
  MODEL_DIR   default: models/bge-m3
  HF_HOME     default: ./hf_cache
  EMBED_HOST  default: 0.0.0.0
  EMBED_PORT  default: 9000
  BATCH_SIZE  default: 32
  DEVICE      default: mps   # å¯è®¾ä¸º cpu / cuda
"""

import os
import sys
import threading
from pathlib import Path
import numpy as np
from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer

# ---------- é…ç½® ----------
MODEL_DIR = Path(os.getenv("MODEL_DIR", "models/bge-m3"))
HF_HOME   = Path(os.getenv("HF_HOME", Path.cwd() / "hf_cache"))
HOST      = os.getenv("EMBED_HOST", "0.0.0.0")
PORT      = int(os.getenv("EMBED_PORT", 9000))
BATCH     = int(os.getenv("BATCH_SIZE", 32))
DEVICE    = os.getenv("DEVICE", "mps")  # M4 Pro ä¼˜å…ˆ MPS

# å»ºè®®å¼€å¯ MPS å›é€€ï¼ˆä¸ªåˆ«ç®—å­ä¸æ”¯æŒæ—¶å›é€€ CPUï¼‰
os.environ.setdefault("PYTORCH_ENABLE_MPS_FALLBACK", "1")

# ---------- è¿è¡Œå‰æ£€æŸ¥ ----------
if not MODEL_DIR.exists() or not any(MODEL_DIR.iterdir()):
    print(f"âŒ Model not found or empty: {MODEL_DIR}")
    print("   è¯·å…ˆä¸‹è½½æ¨¡å‹ï¼Œä¾‹å¦‚ï¼špython docs/models_downloading.py")
    sys.exit(1)

# å›ºå®š HF ç¼“å­˜ç›®å½• & å®Œå…¨ç¦»çº¿
os.environ["HF_HOME"] = str(HF_HOME)
os.environ["HF_HUB_OFFLINE"] = "1"
HF_HOME.mkdir(parents=True, exist_ok=True)

# ---------- Flask ----------
app = Flask(__name__)

ready = False
model = None
EMBED_DIM = 1024  # BGE-M3 ç»´åº¦

def _batch_iter(items, bs):
    for i in range(0, len(items), bs):
        yield items[i:i+bs]

def _load_model_async():
    global model, ready
    print(f"ğŸ”§ Loading SentenceTransformer from: {MODEL_DIR} (device={DEVICE})")
    model = SentenceTransformer(str(MODEL_DIR), device=DEVICE)
    ready = True
    print("âœ… Model loaded and ready.")

@app.get("/healthz")
def healthz():
    return "ok", 200

@app.get("/readyz")
def readyz():
    return ("ready", 200) if ready else ("initializing", 503)

@app.post("/v1/embeddings")
def embeddings():
    if not ready:
        return jsonify({"error": "model not ready"}), 503

    data = request.get_json(force=True) or {}
    inp = data.get("input", [])
    if isinstance(inp, str):
        texts = [inp]
    elif isinstance(inp, list):
        texts = [str(x) for x in inp]
    else:
        return jsonify({"error": "invalid input type"}), 400

    embs_all = []
    for chunk in _batch_iter(texts, BATCH):
        # normalize_embeddings=True -> å·² L2 å½’ä¸€
        vs = model.encode(
            chunk,
            batch_size=min(BATCH, len(chunk)),
            normalize_embeddings=True,
            convert_to_numpy=True,
            show_progress_bar=False,
        )  # shape: [n, 1024]
        embs_all.extend(vs.tolist())

    return jsonify({
        "object": "list",
        "data": [
            {"object": "embedding", "index": i, "embedding": v}
            for i, v in enumerate(embs_all)
        ],
        "model": "BAAI/bge-m3",
    })

if __name__ == "__main__":
    print(f"ğŸš€ Embedding server booting on http://{HOST}:{PORT}")
    print(f"   Model Dir : {MODEL_DIR}")
    print(f"   HF_HOME   : {HF_HOME}")
    print(f"   Offline   : {os.environ.get('HF_HUB_OFFLINE')}")

    # åå°åŠ è½½æ¨¡å‹ï¼Œç«¯å£å…ˆå¼€æ”¾
    threading.Thread(target=_load_model_async, daemon=True).start()
    app.run(host=HOST, port=PORT)
